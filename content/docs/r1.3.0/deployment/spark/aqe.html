

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>2. How To Use Spark Adaptive Query Execution (AQE) in Kyuubi &mdash; Kyuubi 1.3.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Kyuubi Security Overview" href="../../security/index.html" />
    <link rel="prev" title="1. How To Use Spark Dynamic Resource Allocation (DRA) in Kyuubi" href="dynamic_allocation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> Kyuubi
          

          
            
            <img src="../../_static/kyuubi_logo_gray.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p><span class="caption-text">Usage Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/index.html">Quick Start</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Deploying Kyuubi</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#basics">Basics</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#configurations">Configurations</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../settings.html">1. Introduction to the Kyuubi Configurations System</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html">The Engine Configuration Guide</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="dynamic_allocation.html">1. How To Use Spark Dynamic Resource Allocation (DRA) in Kyuubi</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">2. How To Use Spark Adaptive Query Execution (AQE) in Kyuubi</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../security/index.html">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../client/index.html">Client Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../integrations/index.html">Integrations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitor/index.html">Monitoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sql/index.html">SQL References</a></li>
</ul>
<p><span class="caption-text">Kyuubi Insider</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview/index.html">Overview</a></li>
</ul>
<p><span class="caption-text">Contributing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tools/index.html">Develop Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/index.html">Community</a></li>
</ul>
<p><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../appendix/index.html">Appendixes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Kyuubi</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Deploying Kyuubi</a> &raquo;</li>
        
          <li><a href="index.html">The Engine Configuration Guide</a> &raquo;</li>
        
      <li><span class="section-number">2. </span>How To Use Spark Adaptive Query Execution (AQE) in Kyuubi</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/deployment/spark/aqe.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div align=center><p><img alt="../../_images/kyuubi_logo4.png" src="../../_images/kyuubi_logo4.png" /></p>
</div><section id="how-to-use-spark-adaptive-query-execution-aqe-in-kyuubi">
<h1><span class="section-number">2. </span>How To Use Spark Adaptive Query Execution (AQE) in Kyuubi<a class="headerlink" href="#how-to-use-spark-adaptive-query-execution-aqe-in-kyuubi" title="Permalink to this headline">¶</a></h1>
<section id="the-basics-of-aqe">
<h2><span class="section-number">2.1. </span>The Basics of AQE<a class="headerlink" href="#the-basics-of-aqe" title="Permalink to this headline">¶</a></h2>
<p>Spark Adaptive Query Execution (AQE) is a query re-optimization that occurs during query execution.</p>
<p>In terms of technical architecture, the AQE is a framework of dynamic planning and replanning of queries based on runtime statistics,
which supports a variety of optimizations such as,</p>
<ul class="simple">
<li><p>Dynamically Switch Join Strategies</p></li>
<li><p>Dynamically Coalesce Shuffle Partitions</p></li>
<li><p>Dynamically Handle Skew Joins</p></li>
</ul>
<p>In Kyuubi, we strongly recommended that you turn on all capabilities of AQE by default for Kyuubi engines no matter on what platform you run Kyuubi and Spark.</p>
<section id="dynamically-switch-join-strategies">
<h3>Dynamically Switch Join Strategies<a class="headerlink" href="#dynamically-switch-join-strategies" title="Permalink to this headline">¶</a></h3>
<p>Spark supports several join strategies, among which <code class="docutils literal notranslate"><span class="pre">BroadcastHash</span> <span class="pre">Join</span></code> is usually the most performant when any join side fits well in memory. And for this reason, Spark plans a <code class="docutils literal notranslate"><span class="pre">BroadcastHash</span> <span class="pre">Join</span></code> if the estimated size of a join relation is less than the <code class="docutils literal notranslate"><span class="pre">spark.sql.autoBroadcastJoinThreshold</span></code>.</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span><span class="na">spark.sql.autoBroadcastJoinThreshold</span><span class="o">=</span><span class="s">10M</span>
</pre></div>
</div>
<p>Without AQE, the estimated size of join relations comes from the statistics of the original table. It can go wrong in most real-world cases. For example, the join relation is a convergent but composite operation rather than a single table scan. In this case, Spark might not be able to switch the join-strategy to <code class="docutils literal notranslate"><span class="pre">BroadcastHash</span> <span class="pre">Join</span></code>.  While with AQE, we can runtime calculate the size of the composite operation accurately.  And then, Spark now can replan the join strategy unmistakably if the size fits <code class="docutils literal notranslate"><span class="pre">spark.sql.autoBroadcastJoinThreshold</span></code></p>
<div align=center><p><img alt="../../_images/aqe_switch_join.png" src="../../_images/aqe_switch_join.png" /></p>
</div><p align=right>
<em>
<a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">[2] From Databricks Blog</a>
</em>
</p><p>What’s more,  when <code class="docutils literal notranslate"><span class="pre">spark.sql.adaptive.localShuffleReader.enabled=true</span></code> and after converting <code class="docutils literal notranslate"><span class="pre">SortMerge</span> <span class="pre">Join</span></code> to <code class="docutils literal notranslate"><span class="pre">BroadcastHash</span> <span class="pre">Join</span></code>, Spark also does future optimize to reduce the network traffic by converting a regular shuffle to a localized shuffle.</p>
<div align=center><p><img alt="../../_images/localshufflereader.png" src="../../_images/localshufflereader.png" /></p>
</div><p>As shown in the above fig, the local shuffle reader can read all necessary shuffle files from its local storage, actually without performing the shuffle across the network.</p>
<p>The local shuffle reader optimization consists of avoiding shuffle when the <code class="docutils literal notranslate"><span class="pre">SortMerge</span> <span class="pre">Join</span></code> transforms to <code class="docutils literal notranslate"><span class="pre">BroadcastHash</span> <span class="pre">Join</span></code> after applying the AQE rules.</p>
</section>
<section id="dynamically-coalesce-shuffle-partitions">
<h3>Dynamically Coalesce Shuffle Partitions<a class="headerlink" href="#dynamically-coalesce-shuffle-partitions" title="Permalink to this headline">¶</a></h3>
<p>Without this feature, Spark itself could be a small files maker sometimes, especially in a pure SQL way like Kyuubi does, for example,</p>
<ol class="simple">
<li><p>When <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> is set too large compared to the total output size, there comes very small or empty files after a shuffle stage.</p></li>
<li><p>When Spark performs a series of optimized <code class="docutils literal notranslate"><span class="pre">BroadcastHash</span> <span class="pre">Join</span></code> and <code class="docutils literal notranslate"><span class="pre">Union</span></code> together, the final output size for each partition might be reduced by the join conditions. However, the total final output file numbers get to explode.</p></li>
<li><p>Some pipelined jobs with selective filters to produce temporary data.</p></li>
<li><p>e.t.c</p></li>
</ol>
<p>Reading small files leads to very small partitions or tasks. Spark tasks will have worse I/O throughput and tend to suffer more from scheduling overhead and task setup overhead.</p>
<div align=center><p><img alt="../../_images/blog-adaptive-query-execution-2.png" src="../../_images/blog-adaptive-query-execution-2.png" /></p>
</div><p align=right>
<em>
<a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">[2] From Databricks Blog</a>
</em>
</p><p>Combining small partitions saves resources and improves cluster throughput. Spark provides serval ways to handle small file issues, for example, adding an extra shuffle operation on the partition columns with the <code class="docutils literal notranslate"><span class="pre">distribute</span> <span class="pre">by</span></code> clause or using <code class="docutils literal notranslate"><span class="pre">HINT</span></code>[5]. In most scenarios, you need to have a good grasp of your data, Spark jobs, and configurations to apply these solutions case by case. Mostly, the daily used config - <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code> is data-dependent and unchangeable with a single Spark SQL query. For real-life Spark jobs with multiple stages, it’ impossible to use it as one size to fit all.</p>
<p>But with AQE, things become more comfortable for you as Spark will do the partition coalescing automatically.</p>
<div align=center><p><img alt="../../_images/blog-adaptive-query-execution-3.png" src="../../_images/blog-adaptive-query-execution-3.png" /></p>
</div>
<p align=right>
<em>
<a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">[2] From Databricks Blog</a>
</em>
</p><p>It can simplify the tuning of shuffle partition numbers when running Spark SQL queries. You do not need to set a proper shuffle partition number to fit your dataset.</p>
<p>To enable this feature, we need to set the below two configs to true.</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span><span class="na">spark.sql.adaptive.enabled</span><span class="o">=</span><span class="s">true</span>
<span class="na">spark.sql.adaptive.coalescePartitions.enabled</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<section id="other-tips-for-best-practises">
<h4>Other Tips for Best Practises<a class="headerlink" href="#other-tips-for-best-practises" title="Permalink to this headline">¶</a></h4>
<p>For further tuning our Spark jobs with this feature, we also need to be aware of these configs.</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span><span class="na">spark.sql.adaptive.advisoryPartitionSizeInBytes</span><span class="o">=</span><span class="s">128m</span>
<span class="na">spark.sql.adaptive.coalescePartitions.minPartitionNum</span><span class="o">=</span><span class="s">1</span>
<span class="na">spark.sql.adaptive.coalescePartitions.initialPartitionNum</span><span class="o">=</span><span class="s">200</span>
</pre></div>
</div>
<section id="how-to-set-spark-sql-adaptive-advisorypartitionsizeinbytes">
<h5>How to set <code class="docutils literal notranslate"><span class="pre">spark.sql.adaptive.advisoryPartitionSizeInBytes</span></code>?<a class="headerlink" href="#how-to-set-spark-sql-adaptive-advisorypartitionsizeinbytes" title="Permalink to this headline">¶</a></h5>
<p>It stands for the advisory size in bytes of the shuffle partition during adaptive query execution, which takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition. The default value of <code class="docutils literal notranslate"><span class="pre">spark.sql.adaptive.advisoryPartitionSizeInBytes</span></code> is 64M.  Typically, if we are reading and writing data with HDFS, matching it with the block size of HDFS should be the best choice, i.e. 128MB or 256MB.</p>
<p>Consequently, all blocks or partitions in Spark and files in HDFS are chopped up to 128MB/256MB chunks. And think about it, now all tasks for scans, sinks, and middle shuffle maps are dealing with mostly even-sized data partitions. It will make us much easier to set up executor resources or even one size to fit all.</p>
</section>
<section id="how-to-set-spark-sql-adaptive-coalescepartitions-minpartitionnum">
<h5>How to set <code class="docutils literal notranslate"><span class="pre">spark.sql.adaptive.coalescePartitions.minPartitionNum</span></code>?<a class="headerlink" href="#how-to-set-spark-sql-adaptive-coalescepartitions-minpartitionnum" title="Permalink to this headline">¶</a></h5>
<p>It stands for the suggested (not guaranteed) minimum number of shuffle partitions after coalescing. If not set, the default value is the default parallelism of the Spark application. The default parallelism is defined by <code class="docutils literal notranslate"><span class="pre">spark.default.parallelism</span></code> or else the total count of cores registered. I guess the motivation of this behavior made by the Spark community is to maximize the use of the resources and concurrency of the application.</p>
<p>But there are always exceptions. Relating these two seemingly unrelated parameters can be somehow tricky for users. This config is optional by default which means users may not touch it in most real-world cases. But <code class="docutils literal notranslate"><span class="pre">spark.default.parallelism</span></code> has a long history and is well known then. If users set the default parallelism to an illegitimate high value unexpectedly, it could block AQE from coalescing partitions to a fair number. Another scenario that requires special attention is writing data. Usually, coalescing partitions to avoid small file issues is more critical than task concurrency for final output stages. A better data layout can benefit plenty of downstream jobs. I suggest that we set <code class="docutils literal notranslate"><span class="pre">spark.sql.adaptive.coalescePartitions.minPartitionNum</span></code> to 1 in this case as Spark will try its best to but not guaranteed to coalesce partitions for output.</p>
</section>
<section id="how-to-set-spark-sql-adaptive-coalescepartitions-initialpartitionnum">
<h5>How to set <code class="docutils literal notranslate"><span class="pre">spark.sql.adaptive.coalescePartitions.initialPartitionNum</span></code>?<a class="headerlink" href="#how-to-set-spark-sql-adaptive-coalescepartitions-initialpartitionnum" title="Permalink to this headline">¶</a></h5>
<p>It stands for the initial number of shuffle partitions before coalescing. By default it equals to <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions(200)</span></code>. Firstly, it’s better to set it explicitly rather than falling back to <code class="docutils literal notranslate"><span class="pre">spark.sql.shuffle.partitions</span></code>. Spark community suggests set a large number to it as Spark will dynamically coalesce shuffle partitions, which I cannot agree more.</p>
</section>
</section>
</section>
<section id="dynamically-handle-skew-joins">
<h3>Dynamically Handle Skew Joins<a class="headerlink" href="#dynamically-handle-skew-joins" title="Permalink to this headline">¶</a></h3>
<p>Without AQE, the data skewness is very likely to occur for map-reduce computing models in the shuffle phase. Data skewness can cause Spark jobs to have one or more tailing tasks, severely downgrading queries’ performance. This feature dynamically handles skew in <code class="docutils literal notranslate"><span class="pre">SortMerge</span> <span class="pre">Join</span></code> by splitting (and replicating if needed) skewed tasks into roughly evenly sized tasks. For example, The optimization will split oversized partitions into subpartitions and join them to the other join side’s corresponding partition.</p>
<div align=center><p><img alt="../../_images/blog-adaptive-query-execution-6.png" src="../../_images/blog-adaptive-query-execution-6.png" /></p>
</div>
<p align=right>
<em>
<a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">[2] From Databricks Blog</a>
</em>
</p><p>To enable this feature, we need to set the below two configs to true.</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span><span class="na">spark.sql.adaptive.enabled</span><span class="o">=</span><span class="s">true</span>
<span class="na">spark.sql.adaptive.skewJoin.enabled</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<section id="id1">
<h4>Other Tips for Best Practises<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>For further tuning our Spark jobs with this feature, we also need to be aware of these configs.</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span><span class="na">spark.sql.adaptive.skewJoin.skewedPartitionFactor</span><span class="o">=</span><span class="s">5</span>
<span class="na">spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</span><span class="o">=</span><span class="s">256M</span>
<span class="na">spark.sql.adaptive.advisoryPartitionSizeInBytes</span><span class="o">=</span><span class="s">64M</span>
</pre></div>
</div>
<section id="how-to-set-spark-sql-adaptive-skewjoin-skewedpartitionfactor-and-skewedpartitionthresholdinbytes">
<h5>How to set <code class="docutils literal notranslate"><span class="pre">spark.sql.adaptive.skewJoin.skewedPartitionFactor</span></code> and <code class="docutils literal notranslate"><span class="pre">skewedPartitionThresholdInBytes</span></code>?<a class="headerlink" href="#how-to-set-spark-sql-adaptive-skewjoin-skewedpartitionfactor-and-skewedpartitionthresholdinbytes" title="Permalink to this headline">¶</a></h5>
<p>Spark uses these two configs and the median(<strong>not average</strong>) partition size to detect whether a partition skew or not.</p>
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span>partition size &gt; skewedPartitionFactor * the median partition size &amp;&amp; \
skewedPartitionThresholdInBytes
</pre></div>
</div>
<p>As Spark splits skewed partitions targeting <a class="reference external" href="aqe.html#how-to-set-spark-sql-adaptive-advisorypartitionsizeinbytes">spark.sql.adaptive.advisoryPartitionSizeInBytes</a>, ideally <code class="docutils literal notranslate"><span class="pre">skewedPartitionThresholdInBytes</span></code> should be larger than <code class="docutils literal notranslate"><span class="pre">advisoryPartitionSizeInBytes</span></code>. In this case, anytime you increase <code class="docutils literal notranslate"><span class="pre">advisoryPartitionSizeInBytes</span></code>, you should also increase <code class="docutils literal notranslate"><span class="pre">skewedPartitionThresholdInBytes</span></code> if you tend to enable the feature.</p>
</section>
</section>
</section>
<section id="hidden-features">
<h3>Hidden Features<a class="headerlink" href="#hidden-features" title="Permalink to this headline">¶</a></h3>
<section id="demotebroadcasthashjoin">
<h4>DemoteBroadcastHashJoin<a class="headerlink" href="#demotebroadcasthashjoin" title="Permalink to this headline">¶</a></h4>
<p>Internally, Spark has an optimization rule that detects a join child with a high ratio of empty partitions and adds a no-broadcast-hash-join hint to avoid broadcasting it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">adaptive</span><span class="o">.</span><span class="n">nonEmptyPartitionRatioForBroadcastJoin</span><span class="o">=</span><span class="mf">0.2</span>
</pre></div>
</div>
<p>By default, if there are only less than 20% partitions of the dataset contain data, Spark will not broadcast the dataset.</p>
</section>
<section id="eliminatejointoemptyrelation">
<h4>EliminateJoinToEmptyRelation<a class="headerlink" href="#eliminatejointoemptyrelation" title="Permalink to this headline">¶</a></h4>
<p>This optimization rule detects and converts a Join to an empty LocalRelation.</p>
</section>
<section id="disabling-the-hidden-features">
<h4>Disabling the Hidden Features<a class="headerlink" href="#disabling-the-hidden-features" title="Permalink to this headline">¶</a></h4>
<p>We can exclude some of the AQE additional rules if performance regression or bug occurs. For example,</p>
<div class="highlight-sql notranslate"><div class="highlight"><pre><span></span><span class="k">SET</span> <span class="n">spark</span><span class="p">.</span><span class="k">sql</span><span class="p">.</span><span class="n">adaptive</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">excludedRules</span><span class="o">=</span><span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="k">sql</span><span class="p">.</span><span class="n">execution</span><span class="p">.</span><span class="n">adaptive</span><span class="p">.</span><span class="n">DemoteBroadcastHashJoin</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="best-practices-for-applying-aqe-to-kyuubi">
<h2><span class="section-number">2.2. </span>Best Practices for Applying AQE to Kyuubi<a class="headerlink" href="#best-practices-for-applying-aqe-to-kyuubi" title="Permalink to this headline">¶</a></h2>
<p>Kyuubi is a long-running service to make it easier for end-users to use Spark SQL without having much of Spark’s basic knowledge. It is essential to have a basic configuration that works for most scenarios on the server-side.</p>
<section id="setting-default-configurations">
<h3>Setting Default Configurations<a class="headerlink" href="#setting-default-configurations" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="settings.html#via-spark-defaults-conf">Configuring by <code class="docutils literal notranslate"><span class="pre">spark-defaults.conf</span></code></a> at the engine side is the best way to set up Kyuubi with AQE. All engines will be instantiated with AQE enabled.</p>
<p>Here is a config setting that we use in our platform when deploying Kyuubi.</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span>spark.sql.adaptive.enabled=true
spark.sql.adaptive.forceApply=false
spark.sql.adaptive.logLevel=info
spark.sql.adaptive.advisoryPartitionSizeInBytes=256m
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionNum=1
spark.sql.adaptive.coalescePartitions.initialPartitionNum=8192
spark.sql.adaptive.fetchShuffleBlocksInBatch=true
spark.sql.adaptive.localShuffleReader.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.skewJoin.skewedPartitionFactor=5
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=400m
spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin=0.2
spark.sql.adaptive.optimizer.excludedRules
spark.sql.autoBroadcastJoinThreshold=-1
</pre></div>
</div>
<section id="tips">
<h4>Tips<a class="headerlink" href="#tips" title="Permalink to this headline">¶</a></h4>
<p>Turn on AQE by default can significantly improve the user experience.
Other sub-features are all enabled.
<code class="docutils literal notranslate"><span class="pre">advisoryPartitionSizeInBytes</span></code> is targeting the HDFS block size
<code class="docutils literal notranslate"><span class="pre">minPartitionNum</span></code> is set to 1 for the reason of coalescing first.
<code class="docutils literal notranslate"><span class="pre">initialPartitionNum</span></code> has a high value.
Since AQE requires at least one shuffle, ideally, we need to set <code class="docutils literal notranslate"><span class="pre">autoBroadcastJoinThreshold</span></code> to -1 to involving <code class="docutils literal notranslate"><span class="pre">SortMerge</span> <span class="pre">Join</span></code> with a shuffle for all user queries with joins. But then, the  Dynamically Switch Join Strategies feature seems can not be applied later in this case. It appears to be a typo limitation of Spark AQE so far.</p>
</section>
</section>
<section id="dynamically-setting">
<h3>Dynamically Setting<a class="headerlink" href="#dynamically-setting" title="Permalink to this headline">¶</a></h3>
<p>All AQE related configurations are runtime changeable, which means that it can still modify some specific configs by <code class="docutils literal notranslate"><span class="pre">SET</span></code> syntaxes for each SQL query with more precise control on the client-side.</p>
</section>
</section>
<section id="spark-known-issues">
<h2><span class="section-number">2.3. </span>Spark Known issues<a class="headerlink" href="#spark-known-issues" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-33933">SPARK-33933: Broadcast timeout happened unexpectedly in AQE</a></p>
<p>For Spark versions(&lt;3.1), we need to increase <code class="docutils literal notranslate"><span class="pre">spark.sql.broadcastTimeout(300s)</span></code> higher even the broadcast relation is tiny.</p>
<p>For other potential problems that may be found in the AQE features of Spark, you may refer to <a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-33828">SPARK-33828: SQL Adaptive Query Execution QA</a>.</p>
</section>
<section id="references">
<h2><span class="section-number">2.4. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p><a class="reference external" href="https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution">Adaptive Query Execution</a></p></li>
<li><p><a class="reference external" href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">Adaptive Query Execution: Speeding Up Spark SQL at Runtime</a></p></li>
<li><p><a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-31412">SPARK-31412: New Adaptive Query Execution in Spark SQL</a></p></li>
<li><p><a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-28560">SPARK-28560: Optimize shuffle reader to local shuffle reader when smj converted to bhj in adaptive execution</a></p></li>
<li><p><a class="reference external" href="https://issues.apache.org/jira/browse/SPARK-24940">Coalesce and Repartition Hint for SQL Queries</a></p></li>
</ol>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../security/index.html" class="btn btn-neutral float-right" title="Kyuubi Security Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dynamic_allocation.html" class="btn btn-neutral float-left" title="1. How To Use Spark Dynamic Resource Allocation (DRA) in Kyuubi" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the &#34;License&#34;); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.


    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>